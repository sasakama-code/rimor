# ドメイン辞書と文脈理解システムの具体的構築方法

## 1. ドメイン辞書の初期構築戦略

### 1.1 最小限で開始する段階的アプローチ

```yaml
初期辞書の規模:
  Phase 0（MVP）:
    エントリ数: 約100語
    カバー範囲: 基本的なテスト用語のみ
    作成期間: 1週間
    
  Phase 1（基本運用）:
    エントリ数: 約500語
    カバー範囲: 主要なドメイン（Web、API、DB）
    作成期間: 1ヶ月
    
  Phase 2（本格運用）:
    エントリ数: 2000語以上
    カバー範囲: 業界別専門用語を含む
    作成期間: 3ヶ月（継続的拡張）
```

### 1.2 初期辞書の構築方法

**Step 1: 既存資産からの自動抽出**

```bash
# 実際のテストコードから頻出パターンを抽出
$ npx test-quality-audit analyze --extract-patterns ./tests/**/*.test.js

抽出結果:
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
頻出キーワード（テスト名から）:
- "should" (2,341回) → テスト記述の開始語
- "error" (892回) → エラーハンドリング関連
- "return" (734回) → 戻り値チェック
- "valid" (623回) → バリデーション関連

頻出パターン（expect文から）:
- expect().toBe() → 完全一致検証
- expect().toContain() → 部分一致検証
- expect().toThrow() → 例外検証

ドメイン候補語:
- API関連: request, response, endpoint, status
- DB関連: query, transaction, connection
- 認証関連: token, auth, permission, role
```

**Step 2: 業界標準用語の組み込み**

```yaml
標準用語集の活用:
  ソース:
    - ISTQBテスト用語集（公開資料）
    - RFC文書（HTTP、REST API等）
    - 各フレームワークの公式用語
    
  自動インポート例:
    HTTP関連:
      - メソッド: [GET, POST, PUT, DELETE, PATCH]
      - ステータス: {200: OK, 404: NotFound, 500: ServerError}
      - ヘッダー: [Content-Type, Authorization, Accept]
```

### 1.3 実装可能な辞書データ構造

```typescript
// ドメイン辞書の実際の構造
interface DomainDictionary {
  version: string;
  lastUpdated: Date;
  entries: DictionaryEntry[];
}

interface DictionaryEntry {
  // 基本情報
  term: string;                    // 用語
  normalizedForm: string;          // 正規化形（小文字、単数形等）
  category: DomainCategory;        // 分類
  
  // 意味と関連性
  synonyms: string[];              // 同義語
  relatedTerms: string[];          // 関連語
  contextClues: ContextClue[];     // 文脈手がかり
  
  // 使用統計（自動更新）
  frequency: number;               // 使用頻度
  lastSeen: Date;                 // 最終使用日
  confidence: number;              // 信頼度スコア
}

// 実際のエントリ例
const cardMaskingEntry: DictionaryEntry = {
  term: "マスキング",
  normalizedForm: "masking",
  category: "security",
  synonyms: ["マスク", "隠蔽", "非表示", "****表示"],
  relatedTerms: ["暗号化", "ハッシュ化", "トークン化"],
  contextClues: [
    {
      pattern: "カード番号.*マスキング",
      intent: "credit-card-security",
      confidence: 0.95
    },
    {
      pattern: "個人情報.*マスキング",
      intent: "privacy-protection",
      confidence: 0.90
    }
  ],
  frequency: 234,
  lastSeen: new Date("2024-01-15"),
  confidence: 0.88
};
```

## 2. 文脈理解メカニズムの実装

### 2.1 ルールベースの文脈解析

```typescript
// 実装可能な文脈理解エンジン
class ContextAnalyzer {
  private rules: ContextRule[];
  
  constructor() {
    // 初期ルールセット（手動定義）
    this.rules = [
      {
        // ルール: データ保護文脈
        id: "data-protection-context",
        triggers: ["マスキング", "暗号化", "保護", "隠す"],
        conditions: [
          { type: "nearby", terms: ["カード", "個人情報", "パスワード"] },
          { type: "verb", patterns: ["確認", "チェック", "検証"] }
        ],
        conclusion: {
          domain: "security",
          intent: "data-protection-validation"
        }
      },
      {
        // ルール: パフォーマンス文脈
        id: "performance-context",
        triggers: ["レスポンス", "応答", "処理時間"],
        conditions: [
          { type: "number", pattern: /\d+秒|ミリ秒/ },
          { type: "comparison", keywords: ["以下", "超える", "未満"] }
        ],
        conclusion: {
          domain: "performance",
          intent: "response-time-validation"
        }
      }
    ];
  }
  
  analyze(input: string): ContextAnalysisResult {
    const tokens = this.tokenize(input);
    const matchedRules = this.matchRules(tokens);
    
    // 複数ルールがマッチした場合の優先順位付け
    return this.prioritizeResults(matchedRules);
  }
}
```

### 2.2 実際の文脈理解プロセス

```yaml
入力例: "ユーザー登録のAPIで、パスワードが平文で返ってこないことを確認したい"

処理ステップ:
  1. トークン化:
     ["ユーザー登録", "API", "パスワード", "平文", "返って", "こない", "確認"]
  
  2. キーワードマッチング:
     - "API" → API関連文脈
     - "パスワード" → セキュリティ文脈
     - "平文" → セキュリティリスク
     - "確認" → 検証意図
  
  3. 文脈ルール適用:
     トリガー: "パスワード" + "平文"
     条件: 否定形（"こない"）+ 検証動詞（"確認"）
     結論: セキュリティ検証（パスワード非露出）
  
  4. 意図の明確化:
     domain: "api-security"
     intent: "password-exposure-prevention"
     checkType: "negative-assertion"
```

## 3. 初期辞書の実践的構築プロセス

### 3.1 1週間で作る最小限辞書（MVP版）

```yaml
Day 1-2: 基礎用語収集
  作業内容:
    - JestやMochaのドキュメントから基本用語抽出
    - expect, describe, it, test等の予約語
    - toBe, toEqual, toContain等のマッチャー
  成果物: base-terms.json（約50語）

Day 3-4: ドメイン基本用語
  作業内容:
    - HTTP関連: request, response, status, header
    - DB関連: query, insert, update, delete
    - エラー関連: error, exception, throw, catch
  成果物: domain-basics.json（約30語）

Day 5-6: 関連性定義
  作業内容:
    - 同義語グループ作成（例: 削除、delete、remove）
    - 基本的な文脈ルール10個
  成果物: relations.json, basic-rules.json

Day 7: 統合とテスト
  作業内容:
    - 全データの統合
    - 実際の対話例でテスト
  成果物: mvp-dictionary.json（約100語）
```

### 3.2 実際の辞書エントリ例

```json
{
  "entries": [
    {
      "term": "エラーハンドリング",
      "normalizedForm": "error-handling",
      "category": "error-management",
      "synonyms": [
        "エラー処理",
        "例外処理",
        "異常系処理",
        "error handling"
      ],
      "relatedTerms": [
        "try-catch",
        "throw",
        "エラーコード",
        "スタックトレース"
      ],
      "contextClues": [
        {
          "pattern": "エラー.*適切に.*処理",
          "intent": "error-handling-validation",
          "confidence": 0.9
        }
      ],
      "examples": [
        "エラーハンドリングが適切に実装されているか確認",
        "APIのエラーレスポンスをテスト"
      ]
    }
  ]
}
```

## 4. 辞書の継続的改善メカニズム

### 4.1 自動学習システム

```typescript
class DictionaryLearner {
  // ユーザーの対話から新しい用語を学習
  learnFromConversation(dialog: Dialog): void {
    // 未知語の検出
    const unknownTerms = this.findUnknownTerms(dialog);
    
    // 文脈からの意味推定
    unknownTerms.forEach(term => {
      const context = this.extractContext(term, dialog);
      const meaning = this.inferMeaning(term, context);
      
      // 信頼度が閾値を超えたら辞書に追加
      if (meaning.confidence > 0.7) {
        this.addToDictionary(term, meaning);
      } else {
        // 人間によるレビューキューに追加
        this.addToReviewQueue(term, context);
      }
    });
  }
  
  // 実際の使用パターンから関連性を更新
  updateRelations(usage: UsageData): void {
    // 共起頻度の分析
    const cooccurrences = this.analyzeCooccurrence(usage);
    
    // 関連語の更新
    cooccurrences.forEach(pair => {
      if (pair.frequency > threshold) {
        this.addRelation(pair.term1, pair.term2);
      }
    });
  }
}
```

### 4.2 コミュニティによる辞書拡張

```yaml
辞書の共有と拡張:
  公開辞書リポジトリ:
    - 基本辞書（全ユーザー共通）
    - 業界別辞書（金融、医療、EC等）
    - 言語別辞書（日本語、英語等）
  
  貢献方法:
    1. 使用中の自動提案
       "「○○」という用語を辞書に追加しますか？"
    
    2. プルリクエスト
       新規用語や関連性の追加提案
    
    3. 使用統計の共有
       匿名化された使用頻度データ
```

## 5. 実装の現実的なアプローチ

### 5.1 段階的実装計画

```yaml
Phase 0（2週間）- 最小限実装:
  - ハードコードされた100語の辞書
  - 単純なキーワードマッチング
  - 10個の基本的な文脈ルール
  効果: 基本的な対話が可能

Phase 1（1ヶ月）- 基本機能:
  - JSON形式の辞書（500語）
  - ルールベース文脈理解
  - 学習機能の基礎
  効果: 主要なユースケースをカバー

Phase 2（3ヶ月）- 拡張:
  - データベース化（2000語以上）
  - 機械学習による改善
  - コミュニティ機能
  効果: 本格的な運用が可能
```

### 5.2 初期100語で実現できること

```yaml
基本的なテスト品質チェック:
  ✓ "APIのエラーハンドリングをチェック"
  ✓ "レスポンスタイムが3秒以内か確認"
  ✓ "DBのトランザクションが正しくロールバック"
  ✓ "認証トークンの有効期限チェック"
  
これだけでも実用的な理由:
  - 最も頻繁に使われるパターンをカバー
  - 不明な用語は対話で明確化
  - 使用しながら辞書を成長
```

## 6. 辞書作成の実例

### 6.1 実際の作成プロセス（1日目の例）

```bash
# Step 1: 既存テストコードの分析
$ find ./tests -name "*.test.js" | xargs grep -h "describe\|it\|expect" | head -1000 > test-samples.txt

# Step 2: 頻出パターンの抽出
$ cat test-samples.txt | grep -oE '\b\w+\b' | sort | uniq -c | sort -nr | head -50

# 結果例:
# 234 should
# 189 expect
# 156 response
# 134 error
# 98  user
# 87  data
# ...
```

### 6.2 初期辞書の実例（一部）

```json
{
  "version": "0.1.0",
  "entries": [
    {
      "term": "should",
      "category": "test-description",
      "usage": "テストの期待動作を記述",
      "patterns": ["should + 動詞", "should not + 動詞"]
    },
    {
      "term": "response",
      "category": "api",
      "synonyms": ["レスポンス", "応答", "返却値"],
      "relatedTerms": ["request", "status", "body"],
      "contextClues": [
        {
          "pattern": "response.status",
          "meaning": "HTTPステータスコード"
        }
      ]
    }
  ]
}
```

## まとめ：実現可能な辞書システム

ドメイン辞書と文脈理解システムは、以下のアプローチで現実的に構築可能です：

1. **最小限から開始**：100語程度の基本辞書でMVPを構築
2. **既存資産の活用**：実際のテストコードから自動抽出
3. **段階的な拡張**：使用しながら辞書を成長
4. **コミュニティの力**：ユーザーが使うほど賢くなる仕組み
5. **実装の単純さ**：最初はJSONファイルとルールベースで十分

この方法により、初期投資を最小限に抑えながら、実用的な対話型プラグイン作成システムを実現できます。